
### You Only Look Once: Unified, Real-Time Object Detection  
#### Abstract  
我们提出YOLO，一个新的目标检测方法。之前有关对象检测的工作重新利用了分类器来执行检测。我们将对象检测框架化为空间分隔的边界框和相关类概率的回归问题。单个神经网络可以在一次评估中直接从完整图像中预测边界框和类概率。 由于整个检测管道是单个网络，因此可以直接在检测性能上进行端到端优化。  
我们的一体结构非常快。我们基本YOLO模型以45帧/s处理图像。一个小的网络版本，Fast YOLO，以惊人的155帧/s处理，但仍然实现其他实时检测器的双倍mAP（Mean Average Precision, mAP）。相比于目前最好的检测系统，YOLO会产生更多的定位错误，但预测背景的假阳性的可能性较小。最后，YOLO学习非常普遍的对象表示形式。 从自然图像推广到艺术品等其他领域时，它的性能优于其他检测方法，包括DPM和R-CNN。  
#### 1.Introduction  
人们看了一眼图像，立即知道图像中有什么对象，它们在哪里以及它们如何相互作用。人类的视觉系统快速而准确，使我们能够执行一些复杂的任务，例如无意识的驾驶。 快速，准确的对象检测算法将使计算机无需专用传感器即可驾驶汽车，使辅助设备向人类用户传达实时场景信息，并释放通用响应型机器人系统的潜力。  
当前的检测系统重新利用分类器来执行检测。为了检测物体，这些系统采用了该物体的分类器，并在测试图像的各个位置和比例上对其进行了评估。像DPM(deformable parts models)之类的系统使用滑动窗口方法，其中分类器在整个图像上均匀分布的位置运行。  
诸如R-CNN的最新方法使用区域提议方法，首先在图像中生成潜在的边界框，然后在这些提议的框上运行分类器。分类后，使用后期处理来精简边界框，消除重复的检测，并根据场景中的其他对象对框进行重新评分。这些复杂的流程缓慢且难以优化，因为每个单独的组件都必须分别进行训练。  
我们将对象检测重新构造为一个回归问题，直接从图像像素到边界框坐标和类概率。使用我们的系统，您只需看一次（YOLO）图像即可预测存在的物体及其位置。  
![图1](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/Fig1.png)
YOLO非常简单：请参见图1。单个卷积网络可同时预测多个边界框和这些框的类概率。 YOLO训练完整图像并直接优化检测性能。 与传统的对象检测方法相比，此统一模型具有多个优点。  
首先，YOLO非常快。由于我们将检测框架视为回归问题，因此不需要复杂的流程。我们只需在测试时在新图像上运行神经网络即可预测检测结果。我们的基本网络以每秒45帧的速度运行，在Titan X GPU上没有批处理，而快速版本的运行速度超过150 fps。这意味着我们可以以不到25毫秒的延迟实时处理流视频。而且，YOLO达到其他实时系统平均平均精度的两倍以上。有关在网络摄像头上实时运行的系统的演示，请参阅我们的项目网页：http://pjreddie.com/yolo/.   
其次，YOLO在做出预测时会全局考虑图像。与滑动窗口和基于区域提议的技术不同，YOLO在训练和测试期间会看到整个图像，因此它隐式地编码有关类及其外观的上下文信息。快速R-CNN是一种顶部检测方法，因为它看不到较大的上下文，因此将图像中的背景色块误认为是对象。 与Fast R-CNN相比，YOLO产生的背景错误少于一半。  
第三，YOLO学习对象的普遍表示。在自然图像上进行训练并在艺术品上进行测试时，YOLO在很大程度上优于DPM和R-CNN等顶级检测方法。 由于YOLO具有高度通用性，因此在应用于新域或意外输入时，失效的可能性较小。  
YOLO在准确性方面仍落后于最新的检测系统。 尽管它可以快速识别图像中的对象，但仍难以精确定位某些对象，尤其是小的对象。 我们在实验中进一步研究了这些权衡。  
#### 2.Unified Detection  
我们将对象检测的各个组成部分统一为一个神经网络。 我们的网络使用整个图像中的特征来预测每个边界框。 它还可以同时预测图像所有类的所有边界框。 这意味着我们的网络会全局考虑整个图像和图像中的所有对象。 YOLO设计可实现端到端的训练和实时速度，同时保持较高的平均精度。  
我们的系统将输入图像划分为S×S网格。如果对象的中心落入网格单元，则该网格单元负责检测该对象。  
每个网格单元预测B边界框和这些框的置信度得分。这些置信度得分反映出该模型对盒子包含一个对象的确信，以及它认为盒子预测的准确性。 我们正式定义置信度为$\(\operatorname{Pr}(\text { Object }) * \mathrm{IOU}_ {\text {pred }}^{\text {truth }}\)$。 如果该单元格中没有对象，则置信度分数应为零。 否则，我们希望置信度分数等于预测框与ground truth之间的联合相交（IOU）。  
  
##### 换句话说，如果ground truth落在这个grid cell里，那么Pr（Object）就取1，否则就是0，IOU就是bounding box与实际的groud truth之间的交并比。所以confidence就是这两者的乘积。  
![grid](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/grid.png)
##### 一幅图片分成7x7个网格(grid cell)，由网络的最后一层输出7×7×30的tensor，也就是说每个格子输出1×1×30的tensor。30里面包括了2个bound ing box的x，y，w，h，confidengce以及针对格子而言的20个类别概率，输出就是 7x7x(5x2 + 20) 。(通用公式： SxS个网格，每个网格要预测B个bounding box还要预测C个categories，输出就是S x S x (5×B+C)的一个tensor。 注意：class信息是针对每个网格的，confidence信息是针对每个bounding box的）
![tensor](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/tensor.png)
##### 一个grid cell中是否包含object？如果一个object的ground truth的中心点坐标在一个grid cell中，那么这个grid cell就是包含这个object，也就是说这个object的预测就由该grid cell负责。  
  
每个边界框包含5个预测：x，y，w，h和置信度。（x，y）坐标表示框相对于网格单元边界的中心。相对于整个图像预测宽度和高度。 最后，置信度预测表示预测框与任何真实框之间的IOU。  
每个网格单元还可以预测C个条件类别的概率，$\(\operatorname{Pr}\left(\text { Class }_ {i} | \text { Object }\right)\)$.这些概率以包含对象的网格单元为条件。无论框B的数量如何，我们仅预测每个网格单元的一组类概率。无论框B的数量如何，我们仅预测每个网格单元的一组类概率。  
在测试时，我们将条件类别的概率与各个框的置信度预测相乘，$\begin{equation}
\operatorname{Pr}\left(\text { Class }_ {i} | \text { Object }\right) * \operatorname{Pr}(\text { Object }) * \mathrm { IOU}_ {\text {pred }}^{\text {truth}}=\operatorname{Pr}\left(\text { Class }_ {i}\right) * \text { IOU }_ {\text {pred}}^{\text {truth}}
\end{equation}$这为我们提供了每个box的特定类的置信度得分。 这些分数既编码了该类别出现在box中的概率和预测框适合对象的程度。  
![Fig2](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/Fig2.png)  
###### Model.  
我们的系统将检测建模为回归问题。将图像分成SxS的网格并且每个网格预测B个边框，为这些框设置置信度，并C类概率。这些预测编码为$S \times S \times(B * 5+C)$的张量。  
在PASCAL VOC上评价YOLO，设置S=7，B=2。 PASCAL VOC有20个类别标签，所以C=20.所以最终是7X7X30的张量。   
##### 2.1 Network Design  
我们使用卷积神经网络实现这个模型并在PASCAL VOC检测数据集上评估。网络开始的卷积层从图像提取特征，全连接层预测输出概率和坐标。  
我们的网络结构受GoogLeNet图像分类模型启发。我们的网络有24层卷积，紧接着是2层全连接层。我们使用1X1 reduction层接着是3x3卷积层来代替GoogLeNet的残差块。整个网络如图三所示。![Fig3](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/Fig3.png)  
##### Architecture.   
24层卷积层接2层全连接层。交替的1×1卷积层减少了前一层的特征空间。我们以一半的分辨率（224×224输入图像）对ImageNet分类任务预训练卷积层，然后将分辨率提高一倍以进行检测。  
我们还训练了一个快速版本的YOLO，旨在突破快速物体检测的界限。Fast YOLO的神经网络使用更少的卷积层（9层）和更少的卷积核。除了网络的大小之外，YOLO和Fast YOLO的所有训练和测试参数都相同。  
#### 2.2. Training  
我们在ImageNet1000类比赛数据集上预训练卷积层。预训练使用图3的前20层卷积接着一个平均池化层和一个全连接层。我们训练这个网络大约一周并在ImageNet 2012验证集上达到单crop前五名的准确性为88％，可与Caffe的Model Zoo中的GoogLeNet模型相比。我们使用Darknet框架进行所有训练和推理。  
然后，我们将模型转换为执行检测。Ren et al. 等表明将卷积层和连接层都添加到预训练的网络中可以提高性能。按照他们的示例，我们添加了四个卷积层和两个全连接层，它们具有随机初始化的权重。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224×224增加到448×448。  
我们的最后一层可以预测类概率和边界框坐标。我们通过图像的宽度和高度对边界框的宽度和高度进行归一化，使它们落在0和1之间。我们将边界框的x和y坐标参数化为特定网格单元位置的偏移量，因此它们也被限制在0和1之间。  
我们对最终层使用线性激活函数，而所有其他层均使用以下泄漏校正线性激活（leaky rectified linear activation）：
$$
\phi(x)=
\begin{cases}
x,  & \text{if $x$>0} \\\\
0.1x, & \text{otherwise}
\end{cases}
$$
我们针对模型输出中的平方和误差进行了优化。 我们使用平方和误差是因为它易于优化，但是，它与我们实现平均精度最大化的目标并不完全一致。它对定位误差和分类误差的权重相等，这可能不理想。 此外，在每张图像中，许多网格单元都不包含任何对象。这会将这些单元格的“置信度”得分推向零，通常会超过确实包含对象的单元格的梯度。这可能会导致模型不稳定，从而导致训练在早期就出现分歧。  
为了解决这个问题，对于不包含对象的框，我们增加了边界框坐标预测的损失，并减少了置信度预测的损失。使用$\(\lambda_{\text {coord }}\)$和$\(\lambda_{\text {noobj }}\)$完成。设置$\(\lambda_{\text {coord }}=5\)$,$\(\lambda_{\text {coord }}=.5\)$.  
平方和误差也权衡大边框和小边框中的误差。我们的误差度量标准应该反映出，大边框中的小偏差比小边框中的小偏差要小。为了部分解决此问题，我们预测边界框的宽度和高度的平方根，而不是直接预测宽度和高度。  
YOLO预测每个网格单元有多个边界框。 在训练时，我们只希望一个边界框预测器对每个对象负责。我们将一个预测器指定为“负责任的”预测对象，基于预测具有最高的当前IOU。这导致边界框预测器之间的专用化。每个预测器都可以更好地预测某些大小，纵横比或对象类别，从而改善总体召回率。  
在训练阶段，我们优化了以下多部分损失函数：
$$
\begin{aligned}
\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_ {i j}^{\text {obj }}\left[\left(x_{i}-\hat{x}_ {i}\right)^{2}+\left(y_{i}-\hat{y}_ {i}\right)^{2}\right] \\\\
+\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_ {i j}^{\text {obj }}\left[(\sqrt{w_{i}}-\sqrt{\hat{w}_ {i}})^{2}+(\sqrt{h_{i}}-\sqrt{\hat{h}_ {i}})^{2}\right] \\\\
+\sum_{i=0}^{S^{2}} \sum_{j=0}^{B}  \mathbb{1}_ {i j}^{\text {obj }}\left(C_{i}-\hat{C}_ {i}\right)^{2} \\\\
+\lambda_{\text {noobj }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_ {i j}^{\text {noobj }}\left(C_{i}-\hat{C}_ {i}\right)^{2} \\\\
+\sum_{i=0}^{S^{2}} \mathbb{1}_ {i}^{\text {obj }} \sum_{c \in \text { classes }}\left(p_{i}(c)-\hat{p}_ {i}(c)\right)^{2}
\end{aligned}
$$
$\mathbb{1}_ {i}^{\text {obj }}$表示目标是否出现在单元i，$\mathbb{1}_ {ij}^{\text {obj }}$表示单元格i中的第j个边界框预测器对此预测“负责”。  
![loss](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/loss.png)
前面两行表示localization error(即坐标误差)，第一行是box中心坐标(x,y)的预测，第二行为宽和高的预测。这里注意用宽和高的开根号代替原来的宽和高，这样做主要是因为相同的宽和高误差对于小的目标精度影响比大的目标要大。  
第三、四行表示bounding box的confidence损失，就像前面所说的，分成grid cell包含与不包含object两种情况。这里注意下因为每个grid cell包含两个bounding box，所以只有当ground truth 和该网格中的某个bounding box的IOU值最大的时候，才计算这项。  
  
![test](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/test.png)
等式左边第一项就是每个网格预测的类别信息，第二三项就是每个bounding box预测的confidence。这个乘积即encode了预测的box属于某一类的概率，也有该box准确度的信息.  
![inference](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/inference.png)
对每一个网格的每一个bbox执行同样操作： 7x7x2 = 98 bbox （每个bbox既有对应的class信息又有坐标信息）
![TensorValuesInterpretation](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/TensorValuesInterpretation.png)
得到98bbox的信息后，首先对阈值小于0.2的score清零，然后重新排序，最后再用NMS算法去掉重复率较大的bounding box（NMS:针对某一类别，选择得分最大的bounding box，然后计算它和其它bounding box的IOU值，如果IOU大于0.5，说明重复率较大，该得分设为0，如果不大于0.5，则不改；这样一轮后，再选择剩下的score里面最大的那个bounding box，然后计算该bounding box和其它bounding box的IOU，重复以上过程直到最后）。最后每个bounding box的20个score取最大的score，如果这个score大于0，那么这个bounding box就是这个socre对应的类别（矩阵的行），如果小于0，说明这个bounding box里面没有物体，跳过即可。
![bbox](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/bbox.png)
请注意，如果该网格单元中存在对象，则损失函数只会惩罚分类错误（因此，前面讨论过的条件分类概率）。如果该预测器对真值框“负责”（即该网格单元中任何预测器的IOU最高），它也只会惩罚边界框坐标误差。  
我们在PA S C A L VOC 2007和2012的训练集和测试集上训练了135epoch。在2012上测试时我们也包括VOC 2007测试数据作为训练。在整个训练中，我们使用的批次大小为64，动量为0.9，衰减为0.0005。  
我们的学习率安排如下：第一epoch我们将学习率从$10^{-3}$增加到$10^{-2}$。如果我们以较高的学习率开始，则由于不稳定的梯度，我们的模型经常会发散。我们继续以$10^{-2}$训练75epoch，然后以$10^{-3}$训练30epoch，最后以$10{-4}$训练30epoch。  
为了避免过拟合我们使用dropout和扩展数据增强。在第一个连接层之后，速率为.5的dropout层可防止层之间的共适应。对于数据扩充，我们引入了随机缩放和最多转换20％的原始图像大小。我们还将在HSV颜色空间中将图像的曝光和饱和度随机调整至1.5倍。  
#### 2.3. Inference  
就像在训练中一样，预测测试图像的检测仅需要进行一次网络评估。 在PA S C A L VOC上，网络可预测每个图像98个边界框以及每个框的类概率。与基于分类器的方法不同，YOLO只需要进行一次网络评估，因此测试时间非常快。  
网格设计在边界框预测中强制执行空间分集。 通常，很明显，一个对象属于哪个网格单元，并且网络仅为每个对象预测一个框。 但是，一些大对象或多个单元格边界附近的对象可以被多个单元格很好地定位。非极大值抑制可用于修复这些多次检测。 尽管对于R-CNN或DPM而言，对性能并不重要，但非最大抑制会在mAP中增加2-3％。  
#### 2.4. Limitations of YOLO  
由于每个网格单元仅预测两个框且只能具有一个类别，因此YOLO对边界框的预测施加了强大的空间约束。这种空间约束限制了我们的模型可以预测的附近物体的数量。我们的模型很难用于成群出现的小目标（例如成群的鸟）。  
由于我们的模型学会了根据数据预测边界框，因此很难将其推广到具有新的或不同的长宽比或配置的对象。我们的模型还使用相对粗略的特征来预测边界框，因为我们的体系结构具有来自输入图像的多个下采样层。  
最后，虽然我们训练的是近似检测性能的损失函数，但损失函数在小边界框与大边界框中对待错误的方式相同。大框中的小错误通常是良性的，但小框中的小错误对IOU的影响更大。 错误的主要来源是错误的定位。  












