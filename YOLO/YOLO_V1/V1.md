
### You Only Look Once: Unified, Real-Time Object Detection  
#### Abstract  
我们提出YOLO，一个新的目标检测方法。之前有关对象检测的工作重新利用了分类器来执行检测。我们将对象检测框架化为空间分隔的边界框和相关类概率的回归问题。单个神经网络可以在一次评估中直接从完整图像中预测边界框和类概率。 由于整个检测管道是单个网络，因此可以直接在检测性能上进行端到端优化。  
我们的一体结构非常快。我们基本YOLO模型以45帧/s处理图像。一个小的网络版本，Fast YOLO，以惊人的155帧/s处理，但仍然实现其他实时检测器的双倍mAP（Mean Average Precision, mAP）。相比于目前最好的检测系统，YOLO会产生更多的定位错误，但预测背景的假阳性的可能性较小。最后，YOLO学习非常普遍的对象表示形式。 从自然图像推广到艺术品等其他领域时，它的性能优于其他检测方法，包括DPM和R-CNN。  
#### 1.Introduction  
人们看了一眼图像，立即知道图像中有什么对象，它们在哪里以及它们如何相互作用。人类的视觉系统快速而准确，使我们能够执行一些复杂的任务，例如无意识的驾驶。 快速，准确的对象检测算法将使计算机无需专用传感器即可驾驶汽车，使辅助设备向人类用户传达实时场景信息，并释放通用响应型机器人系统的潜力。  
当前的检测系统重新利用分类器来执行检测。为了检测物体，这些系统采用了该物体的分类器，并在测试图像的各个位置和比例上对其进行了评估。像DPM(deformable parts models)之类的系统使用滑动窗口方法，其中分类器在整个图像上均匀分布的位置运行。  
诸如R-CNN的最新方法使用区域提议方法，首先在图像中生成潜在的边界框，然后在这些提议的框上运行分类器。分类后，使用后期处理来精简边界框，消除重复的检测，并根据场景中的其他对象对框进行重新评分。这些复杂的流程缓慢且难以优化，因为每个单独的组件都必须分别进行训练。  
我们将对象检测重新构造为一个回归问题，直接从图像像素到边界框坐标和类概率。使用我们的系统，您只需看一次（YOLO）图像即可预测存在的物体及其位置。  
![图1](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/Fig1.png)
YOLO非常简单：请参见图1。单个卷积网络可同时预测多个边界框和这些框的类概率。 YOLO训练完整图像并直接优化检测性能。 与传统的对象检测方法相比，此统一模型具有多个优点。  
首先，YOLO非常快。由于我们将检测框架视为回归问题，因此不需要复杂的流程。我们只需在测试时在新图像上运行神经网络即可预测检测结果。我们的基本网络以每秒45帧的速度运行，在Titan X GPU上没有批处理，而快速版本的运行速度超过150 fps。这意味着我们可以以不到25毫秒的延迟实时处理流视频。而且，YOLO达到其他实时系统平均平均精度的两倍以上。有关在网络摄像头上实时运行的系统的演示，请参阅我们的项目网页：http://pjreddie.com/yolo/.   
其次，YOLO在做出预测时会全局考虑图像。与滑动窗口和基于区域提议的技术不同，YOLO在训练和测试期间会看到整个图像，因此它隐式地编码有关类及其外观的上下文信息。快速R-CNN是一种顶部检测方法，因为它看不到较大的上下文，因此将图像中的背景色块误认为是对象。 与Fast R-CNN相比，YOLO产生的背景错误少于一半。  
第三，YOLO学习对象的普遍表示。在自然图像上进行训练并在艺术品上进行测试时，YOLO在很大程度上优于DPM和R-CNN等顶级检测方法。 由于YOLO具有高度通用性，因此在应用于新域或意外输入时，失效的可能性较小。  
YOLO在准确性方面仍落后于最新的检测系统。 尽管它可以快速识别图像中的对象，但仍难以精确定位某些对象，尤其是小的对象。 我们在实验中进一步研究了这些权衡。  
#### 2.Unified Detection  
我们将对象检测的各个组成部分统一为一个神经网络。 我们的网络使用整个图像中的特征来预测每个边界框。 它还可以同时预测图像所有类的所有边界框。 这意味着我们的网络会全局考虑整个图像和图像中的所有对象。 YOLO设计可实现端到端的训练和实时速度，同时保持较高的平均精度。  
我们的系统将输入图像划分为S×S网格。如果对象的中心落入网格单元，则该网格单元负责检测该对象。  
每个网格单元预测B边界框和这些框的置信度得分。这些置信度得分反映出该模型对盒子包含一个对象的确信，以及它认为盒子预测的准确性。 我们正式定义置信度为$\(\operatorname{Pr}(\text { Object }) * \mathrm{IOU}_ {\text {pred }}^{\text {truth }}\)$。 如果该单元格中没有对象，则置信度分数应为零。 否则，我们希望置信度分数等于预测框与ground truth之间的联合相交（IOU）。   
每个边界框包含5个预测：x，y，w，h和置信度。（x，y）坐标表示框相对于网格单元边界的中心。相对于整个图像预测宽度和高度。 最后，置信度预测表示预测框与任何真实框之间的IOU。  
每个网格单元还可以预测C个条件类别的概率，$\(\operatorname{Pr}\left(\text { Class }_ {i} | \text { Object }\right)\)$.这些概率以包含对象的网格单元为条件。无论框B的数量如何，我们仅预测每个网格单元的一组类概率。无论框B的数量如何，我们仅预测每个网格单元的一组类概率。  
在测试时，我们将条件类别的概率与各个框的置信度预测相乘，$\begin{equation}
\operatorname{Pr}\left(\text { Class }_ {i} | \text { Object }\right) * \operatorname{Pr}(\text { Object }) * \mathrm { IOU}_ {\text {pred }}^{\text {truth}}=\operatorname{Pr}\left(\text { Class }_ {i}\right) * \text { IOU }_ {\text {pred}}^{\text {truth}}
\end{equation}$这为我们提供了每个box的特定类的置信度得分。 这些分数既编码了该类别出现在box中的概率和预测框适合对象的程度。  
![Fig2](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/Fig2.png)  
###### Model.  
我们的系统将检测建模为回归问题。将图像分成SxS的网格并且每个网格预测B个边框，为这些框设置置信度，并C类概率。这些预测编码为$S \times S \times(B * 5+C)$的张量。  
在PASCAL VOC上评价YOLO，设置S=7，B=2。 PASCAL VOC有20个类别标签，所以C=20.所以最终是7X7X30的张量。   
##### 2.1 Network Design  
我们使用卷积神经网络实现这个模型并在PASCAL VOC检测数据集上评估。网络开始的卷积层从图像提取特征，全连接层预测输出概率和坐标。  
我们的网络结构受GoogLeNet图像分类模型启发。我们的网络有24层卷积，紧接着是2层全连接层。我们使用1X1 reduction层接着是3x3卷积层来代替GoogLeNet的残差块。整个网络如图三所示。![Fig3](https://github.com/David-on-Code/Object-Detection/blob/master/YOLO/YOLO_V1/Fig3.png)  
##### Architecture.   
24层卷积层接2层全连接层。交替的1×1卷积层减少了前一层的特征空间。我们以一半的分辨率（224×224输入图像）对ImageNet分类任务预训练卷积层，然后将分辨率提高一倍以进行检测。  
我们还训练了一个快速版本的YOLO，旨在突破快速物体检测的界限。Fast YOLO的神经网络使用更少的卷积层（9层）和更少的卷积核。除了网络的大小之外，YOLO和Fast YOLO的所有训练和测试参数都相同。  
#### 2.2. Training  
我们在ImageNet1000类比赛数据集上预训练卷积层。预训练使用图3的前20层卷积接着一个平均池化层和一个全连接层。我们训练这个网络大约一周并在ImageNet 2012验证集上达到单crop前五名的准确性为88％，可与Caffe的Model Zoo中的GoogLeNet模型相比。我们使用Darknet框架进行所有训练和推理。  
然后，我们将模型转换为执行检测。Ren et al. 等表明将卷积层和连接层都添加到预训练的网络中可以提高性能。按照他们的示例，我们添加了四个卷积层和两个全连接层，它们具有随机初始化的权重。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224×224增加到448×448。  
我们的最后一层可以预测类概率和边界框坐标。我们通过图像的宽度和高度对边界框的宽度和高度进行归一化，使它们落在0和1之间。我们将边界框的x和y坐标参数化为特定网格单元位置的偏移量，因此它们也被限制在0和1之间。  
我们对最终层使用线性激活函数，而所有其他层均使用以下泄漏校正线性激活（leaky rectified linear activation）：
$$
\phi(x)=\begin{cases}\begin{split}
{x,} & {\text { if } x>0} \\
{0.1 x,} & {\text { otherwise }}\\
\end{split}\end{cases}
$$
$$
f(n) =
\begin{cases}
n/2,  & \text{if $n$ is even} \\\\
3n+1, & \text{if $n$ is odd}
\end{cases}
$$
我们针对模型输出中的平方和误差进行了优化。 我们使用平方和误差是因为它易于优化，但是，它与我们实现平均精度最大化的目标并不完全一致。它对定位误差和分类误差的权重相等，这可能不理想。 此外，在每张图像中，许多网格单元都不包含任何对象。这会将这些单元格的“置信度”得分推向零，通常会超过确实包含对象的单元格的梯度。这可能会导致模型不稳定，从而导致训练在早期就出现分歧。  
为了解决这个问题，对于不包含对象的框，我们增加了边界框坐标预测的损失，并减少了置信度预测的损失。使用$\(\lambda_{\text {coord }}\)$和$\(\lambda_{\text {noobj }}\)$完成。设置$\(\lambda_{\text {coord }}=5\)$,$\(\lambda_{\text {coord }}=.5\)$.  
平方和误差也权衡大边框和小边框中的误差。我们的误差度量标准应该反映出，大边框中的小偏差比小边框中的小偏差要小。为了部分解决此问题，我们预测边界框的宽度和高度的平方根，而不是直接预测宽度和高度。  
YOLO预测每个网格单元有多个边界框。 在训练时，我们只希望一个边界框预测器对每个对象负责。我们将一个预测器指定为“负责任的”预测对象，基于预测具有最高的当前IOU。这导致边界框预测器之间的专用化。每个预测器都可以更好地预测某些大小，纵横比或对象类别，从而改善总体召回率。  
在训练阶段，我们优化了以下多部分损失函数：
$$
\begin{aligned}
\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_ {i j}^{\text {obj }}\left[\left(x_{i}-\hat{x}_ {i}\right)^{2}+\left(y_{i}-\hat{y}_ {i}\right)^{2}\right] \\
+\lambda_{\text {coord }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_ {i j}^{\text {obj }}\left[(\sqrt{w_{i}}-\sqrt{\hat{w}_ {i}})^{2}+(\sqrt{h_{i}}-\sqrt{\hat{h}_ {i}})^{2}\right] \\
+\sum_{i=0}^{S^{2}} \sum_{j=0}^{B}  \mathbb{1}_ {i j}^{\text {obj }}\left(C_{i}-\hat{C}_ {i}\right)^{2} \\
+\lambda_{\text {noobj }} \sum_{i=0}^{S^{2}} \sum_{j=0}^{B} \mathbb{1}_ {i j}^{\text {noobj }}\left(C_{i}-\hat{C}_ {i}\right)^{2} \\
&+\sum_{i=0}^{S^{2}} \mathbb{1}_ {i}^{\text {obj }} \sum_{c \in \text { classes }}\left(p_{i}(c)-\hat{p}_ {i}(c)\right)^{2}
\end{aligned}
$$
$\mathbb{1}_ {i}^{\text {obj }}$表示目标是否出现在单元i，$\mathbb{1}_ {ij}^{\text {obj }}$表示单元格i中的第j个边界框预测器对此预测“负责”。  
请注意，如果该网格单元中存在对象，则损失函数只会惩罚分类错误（因此，前面讨论过的条件分类概率）。 如果该预测变量对地面真值框“负责”（即该网格单元中任何预测变量的IOU最高），它也只会惩罚边界框坐标误差。













